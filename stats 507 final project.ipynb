{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, TrainingArguments, Trainer\n",
    "import torch\n",
    "import json\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "import random\n",
    "from rouge_score import rouge_scorer\n",
    "from sacrebleu import corpus_bleu\n",
    "from language_tool_python import LanguageTool\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import starting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "# Import pre-trained model\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"deep-learning-analytics/triviaqa-t5-base\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"deep-learning-analytics/triviaqa-t5-base\")\n",
    "\n",
    "# Connect to device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Answer:  ['<pad> Washington</s>']\n"
     ]
    }
   ],
   "source": [
    "# Test example question from hugging face model card\n",
    "text = \"What is the capitol of the US\"\n",
    "\n",
    "preprocess_text = text.strip().replace(\"\\n\",\"\")\n",
    "tokenized_text = tokenizer.encode(preprocess_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "outs = model.generate(\n",
    "            tokenized_text,\n",
    "            max_length=10,\n",
    "            num_beams=2,\n",
    "            early_stopping=True\n",
    "           )\n",
    "\n",
    "dec = [tokenizer.decode(ids) for ids in outs]\n",
    "print(\"Predicted Answer: \", dec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions in each category:\n",
      "Entertainment:  802\n",
      "Science:  2340\n",
      "Food:  980\n"
     ]
    }
   ],
   "source": [
    "# Load the data from each of the 3 categories\n",
    "with open('entertainment.json', 'r') as file:\n",
    "    entertainment = json.load(file)\n",
    "with open('science_and_nature.json', 'r') as file:\n",
    "    science = json.load(file)\n",
    "with open('food_and_drink.json', 'r') as file:\n",
    "    food = json.load(file)\n",
    "\n",
    "# Find number of questions in each category\n",
    "print(\"Number of questions in each category:\")\n",
    "print(\"Entertainment: \", len(entertainment))\n",
    "print(\"Science: \", len(science))\n",
    "print(\"Food: \", len(food))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': '_____ in the name of love?',\n",
       " 'answers': ['Stop'],\n",
       " 'category_id': 'ENTERTAINMENT'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keep only categories, questions, and answers\n",
    "def filter_columns(data, columns_to_keep):\n",
    "    return [{column: item[column] for column in columns_to_keep} for item in data]\n",
    "\n",
    "# Define the columns you want to keep\n",
    "columns_to_keep = ['question', 'answers', 'category_id']\n",
    "\n",
    "# Create a dictionary with filtered data\n",
    "trivia_data = {\n",
    "    'entertainment': filter_columns(entertainment, columns_to_keep),\n",
    "    'science': filter_columns(science, columns_to_keep),\n",
    "    'food': filter_columns(food, columns_to_keep)\n",
    "}\n",
    "\n",
    "trivia_data['entertainment'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': '_____ in the name of love?',\n",
       " 'category_id': 'ENTERTAINMENT',\n",
       " 'answer': 'Stop'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keep only the \"first\" answer in answers\n",
    "def keep_first_answer(data):\n",
    "    for item in data:\n",
    "        item['answers'] = item['answers'][0]\n",
    "    return data\n",
    "\n",
    "# Apply the function to the data\n",
    "trivia_data = {key: keep_first_answer(value) for key, value in trivia_data.items()}\n",
    "\n",
    "# Rename answers column to \"answer\"\n",
    "for category in trivia_data:\n",
    "    for item in trivia_data[category]:\n",
    "        item['answer'] = item.pop('answers')\n",
    "\n",
    "trivia_data['entertainment'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': '\"He\\'s So Fine\", \"One Fine Day\" and \"A Love So Fine\" where hits for what fine group?\"',\n",
       " 'category_id': 'ENTERTAINMENT',\n",
       " 'answer': 'The Chiffons'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove questions with underscores (e.g., fill-in-the-blank questions)\n",
    "def remove_fill_in_the_blank(data):\n",
    "    return [item for item in data if re.search(r'[_]+', item['question']) is None]\n",
    "\n",
    "# Apply the function to the data\n",
    "trivia_data = {key: remove_fill_in_the_blank(value) for key, value in trivia_data.items()}\n",
    "\n",
    "trivia_data['entertainment'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions in each category:\n",
      "Entertainment:  754\n",
      "Science:  1663\n",
      "Food:  955\n"
     ]
    }
   ],
   "source": [
    "# Find number of questions in each category\n",
    "print(\"Number of questions in each category:\")\n",
    "print(\"Entertainment: \", len(trivia_data['entertainment']))\n",
    "print(\"Science: \", len(trivia_data['science']))\n",
    "print(\"Food: \", len(trivia_data['food']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': '\"He\\'s So Fine\", \"One Fine Day\" and \"A Love So Fine\" where hits for what fine group?\"',\n",
       " 'category_id': 'ENTERTAINMENT',\n",
       " 'answer': 'The Chiffons'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def synonym_replacement(text):\n",
    "    words = text.split()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        synonyms = wordnet.synsets(word)\n",
    "        if synonyms:\n",
    "            # Choose a random synonym\n",
    "            synonym = random.choice(synonyms).lemmas()[0].name()\n",
    "            new_words.append(synonym if synonym != word else word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "# Add more questions to the dataset by adding duplicates with synonyms as new questions\n",
    "def augment_data(data, num_new_questions):\n",
    "    new_data = []\n",
    "    for item in data:\n",
    "        # Keep the original question\n",
    "        new_data.append(item)\n",
    "        # Generate up to `num_new_questions` augmented questions\n",
    "        for _ in range(num_new_questions):\n",
    "            new_item = item.copy()\n",
    "            new_item['question'] = synonym_replacement(item['question'])\n",
    "            new_data.append(new_item)\n",
    "    \n",
    "    return new_data\n",
    "\n",
    "# Apply the function to the data\n",
    "trivia_data_aug = {key: augment_data(value, num_new_questions=2) for key, value in trivia_data.items()}\n",
    "trivia_data_aug['entertainment'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions in each category:\n",
      "Entertainment:  2262\n",
      "Science:  4989\n",
      "Food:  2865\n"
     ]
    }
   ],
   "source": [
    "# Find number of questions in each category\n",
    "print(\"Number of questions in each category:\")\n",
    "print(\"Entertainment: \", len(trivia_data_aug['entertainment']))\n",
    "print(\"Science: \", len(trivia_data_aug['science']))\n",
    "print(\"Food: \", len(trivia_data_aug['food']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute predictions\n",
    "def generate_predictions(dataset, model, tokenizer, device):\n",
    "    model.eval()  # Put model in evaluation mode\n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    for example in dataset:\n",
    "        # Tokenize inputs\n",
    "        inputs = tokenizer(example[\"question\"], return_tensors=\"pt\", padding=True, truncation=True, max_length=25)\n",
    "        \n",
    "        # Move input tensors to the correct device\n",
    "        input_ids = inputs[\"input_ids\"].to(device)\n",
    "        attention_mask = inputs[\"attention_mask\"].to(device)  # Make sure to include attention_mask if available\n",
    "        \n",
    "        # Generate predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(input_ids, attention_mask=attention_mask, max_length=10, num_beams=5, early_stopping=True)\n",
    "        \n",
    "        # Decode predictions\n",
    "        predicted_answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        true_answer = example[\"answer\"]\n",
    "        \n",
    "        predictions.append(predicted_answer)\n",
    "        references.append(true_answer)\n",
    "    \n",
    "    return predictions, references\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to extract main words from a sentence, excluding stopwords\n",
    "def extract_main_words(text):\n",
    "    # Get the set of English stopwords from NLTK\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Remove punctuation and split into words\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "    # Remove stopwords\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    return set(filtered_words)\n",
    "\n",
    "# Compute accuracy by checking for overlap of main words\n",
    "def compute_accuracy(predictions, references):\n",
    "    correct = 0\n",
    "    total = len(predictions)\n",
    "    \n",
    "    for pred, ref in zip(predictions, references):\n",
    "        pred_words = extract_main_words(pred)\n",
    "        ref_words = extract_main_words(ref)\n",
    "        \n",
    "        # Check for significant overlap of words (at least 50% of reference words matched)\n",
    "        overlap = len(pred_words & ref_words) / max(len(ref_words), 1)\n",
    "        if overlap >= 0.5:\n",
    "            correct += 1\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train categories distribution:\n",
      "category\n",
      "science          3991\n",
      "food             2292\n",
      "entertainment    1809\n",
      "Name: count, dtype: int64\n",
      "Validation categories distribution:\n",
      "category\n",
      "science          499\n",
      "food             287\n",
      "entertainment    226\n",
      "Name: count, dtype: int64\n",
      "Test categories distribution:\n",
      "category\n",
      "science          499\n",
      "food             286\n",
      "entertainment    227\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Add category labels to combined_data\n",
    "combined_data = []\n",
    "for category, questions in trivia_data_aug.items():\n",
    "    for q in questions:\n",
    "        if 'question' in q and 'answer' in q:  # Ensure required fields exist\n",
    "            combined_data.append({\n",
    "                \"question\": q[\"question\"],\n",
    "                \"answer\": q[\"answer\"],\n",
    "                \"category\": category  # Add category for stratification\n",
    "            })\n",
    "\n",
    "# Convert to a DataFrame to facilitate stratified splitting\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(combined_data)\n",
    "\n",
    "# Perform stratified split for train and temp (validation + test)\n",
    "train_df, temp_df = train_test_split(\n",
    "    df, test_size=0.2, stratify=df[\"category\"], random_state=42\n",
    ")\n",
    "\n",
    "# Perform stratified split for validation and test from temp\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df, test_size=0.5, stratify=temp_df[\"category\"], random_state=42\n",
    ")\n",
    "\n",
    "# Convert splits back to Hugging Face Dataset format\n",
    "split_dataset = {\n",
    "    \"train\": Dataset.from_pandas(train_df.reset_index(drop=True)),\n",
    "    \"validation\": Dataset.from_pandas(val_df.reset_index(drop=True)),\n",
    "    \"test\": Dataset.from_pandas(test_df.reset_index(drop=True))\n",
    "}\n",
    "\n",
    "# Verify the splits\n",
    "print(\"Train categories distribution:\")\n",
    "print(train_df[\"category\"].value_counts())\n",
    "\n",
    "print(\"Validation categories distribution:\")\n",
    "print(val_df[\"category\"].value_counts())\n",
    "\n",
    "print(\"Test categories distribution:\")\n",
    "print(test_df[\"category\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f60e79f356dd4a59aa8dbea0c2239472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8092 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8adc037f2eb486989c9a4eee558201e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1012 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c40bd8a74f7c4da99e9bc52287542e41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1012 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Preprocess the data\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples[\"question\"]\n",
    "    targets = examples[\"answer\"]\n",
    "    model_inputs = tokenizer(inputs, max_length=25, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(targets, max_length=10, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_train = split_dataset[\"train\"].map(preprocess_function, batched=True)\n",
    "tokenized_val = split_dataset[\"validation\"].map(preprocess_function, batched=True)\n",
    "tokenized_test = split_dataset[\"test\"].map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model Accuracy: 10.67%\n"
     ]
    }
   ],
   "source": [
    "# Find accuracy of the the original model on the test set\n",
    "original_predictions, original_references = generate_predictions(tokenized_test, model, tokenizer, device)\n",
    "original_accuracy = compute_accuracy(original_predictions, original_references)\n",
    "\n",
    "print(f\"Original Model Accuracy: {original_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c5d914bbcaa4a569bc518d4b5f6e605",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1012 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ab6f2062e6d43b3acb018ec71979547",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.705573081970215, 'eval_runtime': 25.0619, 'eval_samples_per_second': 40.38, 'eval_steps_per_second': 5.067, 'epoch': 1.0}\n",
      "{'loss': 6.778, 'grad_norm': 5.01008939743042, 'learning_rate': 1.0118577075098814e-05, 'epoch': 1.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "492eb542788240128d578ce0bcc6e7fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8893893957138062, 'eval_runtime': 24.9914, 'eval_samples_per_second': 40.494, 'eval_steps_per_second': 5.082, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42efa6a92df14189b4a74adab4f6a107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6683545112609863, 'eval_runtime': 24.984, 'eval_samples_per_second': 40.506, 'eval_steps_per_second': 5.083, 'epoch': 3.0}\n",
      "{'loss': 1.9677, 'grad_norm': 2.9499571323394775, 'learning_rate': 2.3715415019762845e-07, 'epoch': 3.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eb2c4c740fc42e684fd39e71932f961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6194323301315308, 'eval_runtime': 25.002, 'eval_samples_per_second': 40.477, 'eval_steps_per_second': 5.08, 'epoch': 4.0}\n",
      "{'train_runtime': 2404.7365, 'train_samples_per_second': 13.46, 'train_steps_per_second': 0.421, 'train_loss': 4.342598460879722, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1012, training_loss=4.342598460879722, metrics={'train_runtime': 2404.7365, 'train_samples_per_second': 13.46, 'train_steps_per_second': 0.421, 'total_flos': 962438916096000.0, 'train_loss': 4.342598460879722, 'epoch': 4.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "# Define Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 10.67%\n"
     ]
    }
   ],
   "source": [
    "# Find accuracy of resulting model on the test set\n",
    "predictions, references = generate_predictions(tokenized_test, model, tokenizer, device)\n",
    "accuracy = compute_accuracy(predictions, references)\n",
    "\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "{'BLEU': 0.22847667075975242, 'F1': 0.08895476504172156, 'ROUGE': {'rouge1': 0.10779769747161051, 'rouge2': 0.020223978919631094, 'rougeL': 0.10779769747161051}}\n"
     ]
    }
   ],
   "source": [
    "# Compute evaluation metrics\n",
    "def compute_finer_metrics(predictions, references):\n",
    "    # Initialize metrics\n",
    "    rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    bleu_scores = []\n",
    "    f1_scores = []\n",
    "    rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "    \n",
    "    # Compute metrics for each prediction-reference pair\n",
    "    for pred, ref in zip(predictions, references):\n",
    "        # BLEU score (expects list of predictions and references)\n",
    "        bleu_scores.append(corpus_bleu([pred], [[ref]]).score)\n",
    "        \n",
    "        # F1 score (token-level overlap)\n",
    "        pred_tokens = pred.split()\n",
    "        ref_tokens = ref.split()\n",
    "        common_tokens = set(pred_tokens).intersection(ref_tokens)\n",
    "        precision = len(common_tokens) / len(pred_tokens) if pred_tokens else 0\n",
    "        recall = len(common_tokens) / len(ref_tokens) if ref_tokens else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "        f1_scores.append(f1)\n",
    "        \n",
    "        # ROUGE scores\n",
    "        rouge_result = rouge.score(pred, ref)\n",
    "        for key in rouge_scores:\n",
    "            rouge_scores[key].append(rouge_result[key].fmeasure)\n",
    "    \n",
    "    # Calculate averages\n",
    "    avg_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "    avg_f1 = sum(f1_scores) / len(f1_scores)\n",
    "    avg_rouge = {key: sum(scores) / len(scores) for key, scores in rouge_scores.items()}\n",
    "    \n",
    "    return {\n",
    "        \"BLEU\": avg_bleu,\n",
    "        \"F1\": avg_f1,\n",
    "        \"ROUGE\": avg_rouge\n",
    "    }\n",
    "\n",
    "# Compute evaluation metrics\n",
    "metrics = compute_finer_metrics(predictions, references)\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "{'BLEU': 0.09486924458240018, 'F1': 0.09068793525315265, 'ROUGE': {'rouge1': 0.1139196168346366, 'rouge2': 0.02585638998682477, 'rougeL': 0.11359023607047322}}\n"
     ]
    }
   ],
   "source": [
    "# Original metrics\n",
    "metrics = compute_finer_metrics(original_predictions, original_references)\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./trained_model\\\\tokenizer_config.json',\n",
       " './trained_model\\\\special_tokens_map.json',\n",
       " './trained_model\\\\spiece.model',\n",
       " './trained_model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model and tokenizer\n",
    "trainer.save_model(\"./trained_model\")\n",
    "tokenizer.save_pretrained(\"./trained_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"./trained_model\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"./trained_model\")\n",
    "\n",
    "# Put the model in evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to select category\n",
    "def select_category(trivia_data):\n",
    "    categories = [\"entertainment\", \"science\", \"food\"]\n",
    "    print(\"Available categories:\", flush = True)\n",
    "    for i, category in enumerate(categories, 1):\n",
    "        print(f\"{i}. {category}\")\n",
    "    choice = int(input(\"Choose a category by number: \")) - 1\n",
    "    return categories[choice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to select a random question\n",
    "def get_random_question(category, trivia_data):\n",
    "    return random.choice(trivia_data[category])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trivia_game(trivia_data, model, tokenizer):\n",
    "    score_human = 0\n",
    "    score_model = 0\n",
    "    print(\"Welcome to the Trivia Game! It's you vs the AI model! \\n\", flush = True)\n",
    "    \n",
    "    while True:\n",
    "        # Select category and question\n",
    "        category = select_category(trivia_data)\n",
    "        question_data = get_random_question(category, trivia_data)\n",
    "        question = question_data[\"question\"]\n",
    "        correct_answer = question_data[\"answer\"]\n",
    "\n",
    "        # Display question\n",
    "        print(f\"\\nQuestion: {question} \\n\", flush = True)\n",
    "\n",
    "        # Get human's answer\n",
    "        user_answer = input(\"Your answer: \").strip()\n",
    "\n",
    "        # Get model's answer\n",
    "        inputs = tokenizer(question, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "        outputs = model.generate(inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_length=50)\n",
    "        model_answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        # Display the correct answer and model's answer\n",
    "        print(f\"Your Answer: {user_answer}\", flush = True)\n",
    "        print(f\"Model's Answer: {model_answer}\", flush = True)\n",
    "        print(f\"Correct Answer: {correct_answer} \\n\", flush = True)\n",
    "\n",
    "        # Score the human\n",
    "        if compute_accuracy([user_answer], [correct_answer]) == 1:\n",
    "            print(\"You got it right!\", flush = True)\n",
    "            score_human += 1\n",
    "        else:\n",
    "            print(\"You got it wrong.\", flush = True)\n",
    "\n",
    "        # Score the model\n",
    "        if compute_accuracy([model_answer], [correct_answer]) == 1:\n",
    "            print(\"The model got it right! \\n\", flush = True)\n",
    "            score_model += 1\n",
    "        else:\n",
    "            print(\"The model got it wrong. \\n\", flush = True)\n",
    "\n",
    "        # Display current scores\n",
    "        print(f\"Current Scores:\", flush = True)\n",
    "        print(f\"You: {score_human}\", flush = True)\n",
    "        print(f\"Model: {score_model} \\n\", flush = True)\n",
    "\n",
    "        # Ask if the player wants to play again\n",
    "        play_again = input(\"Do you want to play again? (yes/no): \").strip().lower()\n",
    "        print(\"\\n\", flush = True)\n",
    "        if play_again != \"yes\":\n",
    "            break\n",
    "\n",
    "    # Final scores\n",
    "    print(f\"\\nGame Over! Final Scores:\", flush = True)\n",
    "    print(f\"You: {score_human}\", flush = True)\n",
    "    print(f\"Model: {score_model}\", flush = True)\n",
    "    if score_human > score_model:\n",
    "        print(\"Congratulations! You beat the AI!\", flush = True)\n",
    "    elif score_human < score_model:\n",
    "        print(\"The AI wins! Better luck next time!\", flush = True)\n",
    "    else:\n",
    "        print(\"It's a tie!\", flush = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Trivia Game! It's you vs the AI model! \n",
      "\n",
      "Available categories:\n",
      "1. entertainment\n",
      "2. science\n",
      "3. food\n",
      "\n",
      "Question: What is the name of the whale that swallowed Pinocchio. \n",
      "\n",
      "Your Answer: monstro\n",
      "Model's Answer: Agusta\n",
      "Correct Answer: Monstro \n",
      "\n",
      "You got it right!\n",
      "The model got it wrong. \n",
      "\n",
      "Current Scores:\n",
      "You: 1\n",
      "Model: 0 \n",
      "\n",
      "\n",
      "\n",
      "Available categories:\n",
      "1. entertainment\n",
      "2. science\n",
      "3. food\n",
      "\n",
      "Question: What Element Is Used In The Process Of Galvanisation \n",
      "\n",
      "Your Answer: zinc\n",
      "Model's Answer: Carbon\n",
      "Correct Answer: Zinc \n",
      "\n",
      "You got it right!\n",
      "The model got it wrong. \n",
      "\n",
      "Current Scores:\n",
      "You: 2\n",
      "Model: 0 \n",
      "\n",
      "\n",
      "\n",
      "Available categories:\n",
      "1. entertainment\n",
      "2. science\n",
      "3. food\n",
      "\n",
      "Question: Which country would you associate with the dish Couscous? \n",
      "\n",
      "Your Answer: morocco\n",
      "Model's Answer: Tunisia\n",
      "Correct Answer: TunisiaÂ  \n",
      "\n",
      "You got it wrong.\n",
      "The model got it right! \n",
      "\n",
      "Current Scores:\n",
      "You: 2\n",
      "Model: 1 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Game Over! Final Scores:\n",
      "You: 2\n",
      "Model: 1\n",
      "Congratulations! You beat the AI!\n"
     ]
    }
   ],
   "source": [
    "# Play the trivia game\n",
    "trivia_game(trivia_data, model, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
